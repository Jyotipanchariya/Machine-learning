{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0ad61d",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67097a37",
   "metadata": {},
   "source": [
    "A voting ensemble also known as a voting classifier or majority voting, is a machine learning techniques that combines the predictions of multiple individual classifier to make a final predictions. It is popular method for improving the accurracy and robustness of classifictaion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651f489",
   "metadata": {},
   "source": [
    "The ideas behind a voting ensemble is that by a aggregrating the predictions of multiple models the ensembles can benefits from the diversity of opinion and capture different aspects of the data, leading to more accurate predications. The ensemble combines the individual predictions through a voting scheme where each model's prediction is trated as a vote , and the majority predictions is choosen as the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98abce",
   "metadata": {},
   "source": [
    " There are two main types of voting ensembles :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23312a6",
   "metadata": {},
   "source": [
    "(1). Hard Voting: In this each individual classifier in the ensemble predicts a class label and the class label that recieves the majority of votes is selected as the final prediction. For ex- if there are three classifier in the ensembles and two of them predict class A while one predicts class B , the final predictions would be class B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56391248",
   "metadata": {},
   "source": [
    "(2). Soft Voting: In soft voting , instead of predicting just the class label, each individual classifier assigns probabilities to each class . The probabilities from each classifier are averaged or summed, and the class with the highest avearge probablility is selected as the final predictions . This method takes into account the confidence of each classifier prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227007c",
   "metadata": {},
   "source": [
    "Voting ensembles can be implemented using different types of classifier , such as decision tree , support vector machine, logistic regression or neural networks. The individual classifier can have different architecture , hyperparameters, or even be trained on different subsets of the data . This diversity help to ensure that the ensemble captures different perspectives and reduce the risk of overfitting1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f0a92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dc48f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a synthentic classification datasets\n",
    "X, y =make_classification(n_samples=1000, n_features=10 , random_state=42)\n",
    "\n",
    "#split the datasets into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3f3ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "svm_clf= SVC(probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bfc16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the voting ensemble\n",
    "voting_clf= VotingClassifier(\n",
    "     estimators=[('tree', tree_clf), ('log_reg', log_reg),('svm', svm_clf)],\n",
    "     voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c16367aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the ensemble\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on the testing set\n",
    "y_pred= voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a022391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting ensemble: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"voting ensemble:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b82b2",
   "metadata": {},
   "source": [
    "# In which scenerio I can use voting ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294f749",
   "metadata": {},
   "source": [
    " It can be useful in avrious scenarios where you have multiple classifiers and want to combine their predictions to make a final decision. Here are some scenarios where voting ensembles can be particularly beneficial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63866324",
   "metadata": {},
   "source": [
    " (1). Model Diversity: When you have different classifiers that capture different aspects of the data or have different strengths and weakness , a voting ensembles can have leverage thier diversity . For ex-- if you have a decision tree classifier , a logistic regression classifier , and a support vector machine combining their predictions through a voting ensemble can help capture different patterns and improve overall performance   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a57a4",
   "metadata": {},
   "source": [
    " (2). Robustness: Voting ensembles can improve robustness by reducing the impact of individual classifier errors . If some c;assifier in the ensemble make incorrect predictions due to noise or outliers in the data , the ensemble can still provide accurate results by relying on the majority vote . This can be particularly useful in scenarios where the individual classifiers may have high variance or are sensitive to specific types of errors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fbf47",
   "metadata": {},
   "source": [
    "(3).Model Selection:It can simplify the model selection process by incorporating multiple models without extensive hyperparameter tuning. Instead of choosing a single model , you can combine multiple models models and let the ensembles determine the final predictions . This can save time and effort in fine-tuning individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b094a6",
   "metadata": {},
   "source": [
    "(4). Ensemble Learning : They are a fundamental technique in ensemble learning , which sims to combine multiple models to improve predictive performance . It is popular approch when you have limited resources or time for extensive model development . Voting ensemble provide a simple yet effective way to create an ensemble of models without significant additional complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932d3ee",
   "metadata": {},
   "source": [
    " (5). Handling Uncertanity: Soft voting ensemble , where probabilities or confidence scores are used for prediction can be benficial when dealing with uncertain or ambigous cases. By averaging or summing the probablities from individual classifiers, the ensemble can make more informed decisions and provide more relaible predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f7a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
