{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3f379a",
   "metadata": {},
   "source": [
    "# There are several types of boosting algorthims commonly used in ensemble learning. Some of the popular ones include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bec173",
   "metadata": {},
   "source": [
    " (1). ADA BOOST(Adaptive boosting):\n",
    "AdaBoost is one of the earliest and most well known boosting algorthims. It assigns higher weights to misclassified instances and focus in those instances during subsequent iteration. It sequentially trains a series of weak learners and combines their predictions to form the final ensemble. Adaboost is primarily used for binary classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97cd99",
   "metadata": {},
   "source": [
    " (2). Gradient Boosting: It builds an ensemble of weak learners in a stage wise manner. Each subsequent model is trained to correct the mistakes made by the previous models by fiiting the negative gradient of a loss function. It can handle both classification and regression tasks and is often used with decision trees as base learner. Example of gradient boosting algorthims include XGBOOST , LightGBM, and Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f411c55",
   "metadata": {},
   "source": [
    " (3).XGBoost(Extreme Gradient Boosting ): XGBoost is an optimized implementation of gradient boosting that offers several enhacements , including parallel processing, regularization teachniques, and handling missing values . It uses a combination of tree based model and linear for boosting which allow it to capture both linear and non- linear relationship in the data effieicently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1442693",
   "metadata": {},
   "source": [
    "(4). LightGBM : LightGBM is another gradient boosting framework that focuses on chieving faster training speed and lower memory usage. It uses a novel tree growing algorithms  called \"Gradient - based One side sampling\"(GOSS) to select the most informarive instances for builiding decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b08ee",
   "metadata": {},
   "source": [
    " (5). CatBoost : Catboost is a gradient boosting algorthims that is desgined to handle categorical features directly without need for extensive data preprocessing . It incoorporatess an innovative method to handle categorical variable which include applying a combination of ordered boosting, random permutations and symmetric trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a276b99",
   "metadata": {},
   "source": [
    " (6). Stochastic Gradient Boosting: It introduces into the boosting process by subsampling the training data or features at each iteration. It helps to redice overfitting and can improve the model's generalization ability, especially when dealing with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6a41d",
   "metadata": {},
   "source": [
    "#    How to decide which boosting algorithm type i  have to use in which scenerio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299410f3",
   "metadata": {},
   "source": [
    "Deciding which boosting algorthim to use in a specific scenario depends on several factores . Here are some guidelines to help you make on decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfd8ae",
   "metadata": {},
   "source": [
    "(1). Problem Type: Consider whether you are working on a classification or regression problem. Some boosting algorthims are specifically designed for binary classification tasks, while others can handle both classification and regression . For example , Adaboost is primarily used for binary classification while XGboost and LightGBM are versatile and can be used for the both classification and regression tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770478d",
   "metadata": {},
   "source": [
    "(2). Dataset Size: Take into account the sie of your dataset . If you have a large datastet, algorthims like LightGBM or Catboost that are optimized for faster traning speed and lower memory usage can be benefical . They utilize technique such as data subsampling or features subsampling to handle large datasets more efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e212f",
   "metadata": {},
   "source": [
    "(3). DataSet Complexity: Consider the complexity of your dataset and the relationship within it. If your dataset contains a mix of categorical and numerical features, CatBoost might be good choice as it handle categorical variables directly without the need for extensive preprocessing. On the other hand , if you have a dataset wth complex patterns and non - linear relationship , algorthims like XGBoost or LightGBM which use a combination of tree based models and linear models , may be more suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087273a",
   "metadata": {},
   "source": [
    " (4). Interpretability: Think about the interpretability of the model . If interpretability is important in your scenairo , algorthims like AdaBoost or decision tree based boosting  algorthim(e.g. XGBoost , LightGBM ) provide more transparent model compared to more complex model like natural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ccbe1b",
   "metadata": {},
   "source": [
    "(5).Performance and Tunability : Consider the performance and tunability requriment of your task. Different boosting algorthims may have different default hyperparameter settings and may requried specific tuning approches . Some algorthims like XGBoost and LightGBM offer extensive options for hyperparameter tuning which can be advantageous if you have the time and computational resources for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b4430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\jyoti\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jyoti\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\jyoti\\anaconda3\\lib\\site-packages (from xgboost) (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11352787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55fc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genearting a synthentic classification datsdets\n",
    "X, y= make_classification(n_samples=1000 , n_features=10 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db591a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee58700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(n_estimators=100, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(n_estimators=100, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and training an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c222621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and training  an XGBoost classifier\n",
    "xgboost= XGBClassifier(n_estimators =100, random_state=42)\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40ed4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set for Adaboost\n",
    "y_pred_adaboost= adaboost.predict(X_test)\n",
    "\n",
    "# Making predictions on the test set for XGBoost\n",
    "y_pred_xgboost= xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40494785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.85\n",
      "XGBoost Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy on Adaboost\n",
    "accuracy_adaboost= accuracy_score(y_test, y_pred_adaboost)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_adaboost)\n",
    "\n",
    "\n",
    "accuracy_xgboost= accuracy_score(y_test, y_pred_xgboost)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b7ce8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d39343bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2447e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b87b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genearating a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0974063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c96445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_classifier= GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "accuracy_gb= accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Classifier Accuracy:\",accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d6a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 388, number of negative: 412\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485000 -> initscore=-0.060018\n",
      "[LightGBM] [Info] Start training from score -0.060018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Classifier Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Classifier\n",
    "lgb_classifier = LGBMClassifier(n_estimators=100 , random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(X_test)\n",
    "accuracy_lgb= accuracy_score(y_test, y_pred_gb)\n",
    "print(\"LightGBM Classifier Accuracy:\",accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00138588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "# CatBoost Classifier\n",
    "cat_classifier = CatBoostClassifier(n_estimators=100 , random_state=42 , verbose=0)\n",
    "cat_classifier.fit(X_train, y_train)\n",
    "y_pred_cat = cat_classifier.predict(X_test)\n",
    "accuracy_cat= accuracy_score(y_test, y_pred_cat)\n",
    "print(\"CatBoost Classifier Accuracy:\",accuracy_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d0ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Boosting Classifier Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classifier\n",
    "stoch_gb_classifier = HistGradientBoostingClassifier(max_iter=100 , random_state=42 )\n",
    "stoch_gb_classifier.fit(X_train, y_train)\n",
    "y_pred_stoch_gb = stoch_gb_classifier.predict(X_test)\n",
    "accuracy_stoch_gb= accuracy_score(y_test, y_pred_cat)\n",
    "print(\"Stochastic Gradient Boosting Classifier Accuracy:\",accuracy_stoch_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ce7f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor (for demonstration purposes)\n",
    "gb_regressor= GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "y_pred_gb_regressor = gb_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236de67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
